// Copyright 2020 The IREE Authors
//
// Licensed under the Apache License v2.0 with LLVM Exceptions.
// See https://llvm.org/LICENSE.txt for license information.
// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception

#include "iree-amd-aie/Transforms/KernelDispatch.h"

#include "iree/compiler/Codegen/Utils/CPUUtils.h"
#include "mlir/Dialect/MemRef/IR/MemRef.h"
#include "mlir/Dialect/MemRef/Transforms/Transforms.h"
#include "mlir/Transforms/GreedyPatternRewriteDriver.h"

#include "flexmlMetadata/yamlUtils/YamlLoader.h"
#include "flexmlMetadata/abstraction/Kernel.h"
#include <iostream>
#include <fstream>
#include <sstream>

#define DEBUG_TYPE "kernel-dispatch"

namespace mlir::iree_compiler::AMDAIE {

/// Sets the lowering configuration for dispatch region with root op that
/// implements the contraction operation interface.
static LogicalResult setRootConfig(func::FuncOp entryPointFn,
                                   linalg::MatmulOp matmulOp,
                                   StringRef tilingStrategy) {
  assert(!getLoweringConfig(matmulOp) && "expected lowering_config is not set");
  auto linalgOp = cast<linalg::LinalgOp>(matmulOp.getOperation());
  unsigned numLoops = linalgOp.getNumLoops();
  {
    SmallVector<unsigned> dims;
    linalgOp.getReductionDims(dims);
    if (dims.size() != 1 || dims[0] != numLoops - 1) {
      return matmulOp.emitOpError(
          "expected to have exactly one reduction dim, and it is the innermost "
          "dim");
    }
  }
  std::cout << "Before loadYaml" << std::endl;
  std::ifstream in("/proj/rdi/staff/jornt/versal/iree-amd-aie/tests/samples/GemmFill.yaml");
  std::ostringstream sstr;
  sstr << in.rdbuf();
  auto yaml_str = sstr.str();

  // std::string path = "/proj/rdi/staff/jornt/versal/iree-amd-aie/tests/samples/GemmFill.yaml";
  auto yaml_node = flexmlMetadata::utils::loadYaml(yaml_str);
  std::unordered_map<std::string, flexmlMetadata::abstraction::Kernel> db;
  auto kernel = flexmlMetadata::abstraction::Kernel(yaml_str, db);
  auto m_defined = kernel.getMemberAs<flexmlMetadata::abstraction::Defined>("tiling_recipes.spatial_N.aie_tiling.iteration_domains.M.size");
  auto m_value = m_defined.value();
  auto n_value = kernel.getMemberAs<flexmlMetadata::abstraction::Defined>("tiling_recipes.spatial_N.aie_tiling.iteration_domains.N.size").value();
  auto k_value = kernel.getMemberAs<flexmlMetadata::abstraction::Defined>("tiling_recipes.spatial_N.aie_tiling.iteration_domains.K.size").value();
  std::cout << "After Kernel: " << m_value.index() << std::endl;
  std::cout << "After Kernel: " << m_defined << std::endl;
  std::cout << "After Kernel: " << std::get<uint64_t>(m_value) << std::endl;
  std::cout << "After loadYaml" << std::endl;
  // TODO (nmeshram) : This needs to be moved in a separate more generalized
  // logic. Also, need a flag to experiment between pad based and pack based
  // approach which will have different tile sizes and pass pipelines
  TileSizesListType tileSizes;
  if (tilingStrategy == "ukernel") {
    SmallVector<int64_t> TileSizeLevel0 = {
      (int64_t) std::get<std::uint64_t>(m_value),
      (int64_t) std::get<std::uint64_t>(n_value)
    };
    SmallVector<int64_t> TileSizeLevel1 = {0, 0, (int64_t) std::get<std::uint64_t>(k_value)};
    SmallVector<int64_t> TileSizeLevel2 = {1, 1};
    tileSizes = {TileSizeLevel0, TileSizeLevel1, TileSizeLevel2};
  } else {
    SmallVector<int64_t> TileSizeLevel0 = {8, 8};
    SmallVector<int64_t> TileSizeLevel1 = {4, 4};
    SmallVector<int64_t> TileSizeLevel2 = {0, 0, 4};
    tileSizes = {TileSizeLevel0, TileSizeLevel1, TileSizeLevel2};
  }

  return setOpConfigAndEntryPointFnTranslation(
      entryPointFn, matmulOp, tileSizes,
      IREE::Codegen::DispatchLoweringPassPipeline::CPUDefault);
}

/// Redirects to methods that set the configuration based on operation type.
static LogicalResult setRootConfigImpl(func::FuncOp entryPointFn, Operation *op,
                                       StringRef tilingStrategy) {
  auto setRootConfigFn = [&](Operation *op) -> LogicalResult {
    return TypeSwitch<Operation *, LogicalResult>(op)
        // TODO (nmeshram): This is very limited for now, plan is to
        // let it first crash for all the other ops and then consiously
        // add support for them, this way we can verify our work.
        .Case<linalg::MatmulOp>([&](auto op) {
          return setRootConfig(entryPointFn, op, tilingStrategy);
        })
        .Default([&](Operation *op) { return success(); });
  };
  return setRootConfigFn(op);
}

/// Sets the translation information to use for a dispatch region.
static LogicalResult setTranslationInfoAndRootConfig(
    func::FuncOp entryPointFn, ArrayRef<Operation *> computeOps,
    StringRef tilingStrategy) {
  // Make sure that lowering_config is not preset on any compute ops.
  for (auto computeOp : computeOps) {
    if (getLoweringConfig(computeOp)) return failure();
  }

  FailureOr<Operation *> rootOp = getRootOperation(computeOps);
  if (failed(rootOp)) return failure();
  Operation *rootOperation = rootOp.value();

  // TODO (nmeshram): Handle the case with no known root operation.
  if (!rootOperation) {
    return entryPointFn.emitError("Case with no root ops not yet supported.");
  }

  if (failed(setRootConfigImpl(entryPointFn, rootOperation, tilingStrategy))) {
    return failure();
  }

  // TODO (nmeshram): // Set vector level tile sizes for other operations
  // individually.

  return success();
}

LogicalResult initAIELaunchConfig(ModuleOp moduleOp, StringRef tilingStrategy) {
  llvm::StringMap<IREE::HAL::ExecutableExportOp> exportOps =
      getAllEntryPoints(moduleOp);
  for (auto funcOp : moduleOp.getOps<func::FuncOp>()) {
    auto exportOp = exportOps.lookup(funcOp.getName());
    if (!exportOp) continue;
    if (getTranslationInfo(exportOp)) continue;

    // TODO (nmeshram): Need a default pipeline for control flow cases.
    if (funcOp.getBody().empty() || !llvm::hasSingleElement(funcOp.getBody())) {
      return funcOp.emitError("control flow not yet supported.");
    }

    SmallVector<Operation *> computeOps = getComputeOps(funcOp);
    if (failed(setTranslationInfoAndRootConfig(funcOp, computeOps,
                                               tilingStrategy))) {
      return failure();
    }
  }

  // The root configuration setting introduces `tensor.dim` operations.
  // Resolve those away.
  RewritePatternSet patterns(moduleOp.getContext());
  memref::populateResolveRankedShapedTypeResultDimsPatterns(patterns);
  return applyPatternsAndFoldGreedily(moduleOp, std::move(patterns));
}

}  // namespace mlir::iree_compiler::AMDAIE
